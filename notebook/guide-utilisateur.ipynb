{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "southern-criminal",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <h1 style=\"float: left;\">Guide d'utilisation</h1>\n",
    "    <img style=\"width:20%; height: auto;\" src=\"img/CelesteLogo.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "straight-subscriber",
   "metadata": {},
   "source": [
    "<p>\n",
    "    Celeste est un jeu de type \"die and retry\" où il faut se déplacer parmi de multiples obstacles afin d'arriver à la fin du niveau pour passer au suivant.\n",
    "    La version GBA de Celeste est une version simplifiée se basant sur la machine virtuelle PICO-8 comptant 30 niveaux.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "    Notre objectif était de développer une IA pouvant jouer au jeu d'elle-même en essayant de s'améliorer au fil des essais.\n",
    "    Pour cela nous avons utilisé Gym-Retro, une bibliothèque d'Open IA se basant sur Gym permettant de créer facilement un environnement dédié au jeux rétro pour développer une IA.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amino-appeal",
   "metadata": {},
   "source": [
    "<h1> Installation des outils </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "vocational-sheffield",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym-retro in c:\\python38\\lib\\site-packages (0.8.0)\n",
      "Requirement already satisfied: gym in c:\\users\\romai\\documents\\cours\\m1\\ter\\test gym\\gym (from gym-retro) (0.18.0)\n",
      "Requirement already satisfied: pyglet==1.*,>=1.3.2 in c:\\python38\\lib\\site-packages (from gym-retro) (1.5.0)\n",
      "Requirement already satisfied: future in c:\\python38\\lib\\site-packages (from pyglet==1.*,>=1.3.2->gym-retro) (0.18.2)\n",
      "Requirement already satisfied: scipy in c:\\python38\\lib\\site-packages (from gym->gym-retro) (1.6.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in c:\\python38\\lib\\site-packages (from gym->gym-retro) (1.20.0)\n",
      "Requirement already satisfied: Pillow<=7.2.0 in c:\\python38\\lib\\site-packages (from gym->gym-retro) (7.2.0)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in c:\\python38\\lib\\site-packages (from gym->gym-retro) (1.6.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.0.1; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the 'c:\\python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym-retro in c:\\python38\\lib\\site-packages (0.8.0)\n",
      "Requirement already satisfied: pyglet==1.*,>=1.3.2 in c:\\python38\\lib\\site-packages (from gym-retro) (1.5.0)\n",
      "Requirement already satisfied: gym in c:\\users\\romai\\documents\\cours\\m1\\ter\\test gym\\gym (from gym-retro) (0.18.0)\n",
      "Requirement already satisfied: future in c:\\python38\\lib\\site-packages (from pyglet==1.*,>=1.3.2->gym-retro) (0.18.2)\n",
      "Requirement already satisfied: scipy in c:\\python38\\lib\\site-packages (from gym->gym-retro) (1.6.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in c:\\python38\\lib\\site-packages (from gym->gym-retro) (1.20.0)\n",
      "Requirement already satisfied: Pillow<=7.2.0 in c:\\python38\\lib\\site-packages (from gym->gym-retro) (7.2.0)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in c:\\python38\\lib\\site-packages (from gym->gym-retro) (1.6.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.0.1; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the 'c:\\python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install gym-retro\n",
    "!pip3 install gym-retro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serial-intervention",
   "metadata": {},
   "source": [
    "<h1> L'agent Random </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "missing-antenna",
   "metadata": {},
   "outputs": [],
   "source": [
    "import retro\n",
    "\n",
    "env = retro.make(game='Celeste-GBA', state='Level1')\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    obs, rew, done, info =  env.step(env.action_space.sample())\n",
    "    env.render()\n",
    "    if done:\n",
    "        obs = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informational-conference",
   "metadata": {},
   "source": [
    "Dans un premier temps, nous avons implémenté un agent random appuyant de manière aléatoire sur tous les boutons disponibles. \n",
    "Pour cela, il a fallu créer un dossier nommé 'Celeste-GBA' dans le dossier `data/stable` de Gym-Retro.\n",
    "Ce dossier nécéssite 6 fichiers:\n",
    "- La rom du jeu\n",
    "- Le hash du jeu (généré par la fonction de hashage SHA-1)\n",
    "- Le data.json correspondant à la création des variables et à l'affectation de leur adresse mémoire ainsi que leur type.\n",
    "- Le level.state qui contient l'état initial du jeu. (Généré sur Gym-Retro intégration)\n",
    "- Le metadata.json correspondant au level.state à charger.\n",
    "- Le scenario.json qui contient l'affectation de toutes les rewards et les limites d'un épisode.\n",
    "\n",
    "Un épisode correspond une itération d'apprentissage de l'agent. Il se termine quand une des conditions définie dans le scenario.json est remplie."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split-giving",
   "metadata": {},
   "source": [
    "<h1> Adaptation d'un agent développé pour Pong </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "technological-omega",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Trains an agent with (stochastic) Policy Gradients on Pong. Uses OpenAI Gym. \"\"\"\n",
    "import numpy as np\n",
    "import _pickle as pickle\n",
    "import retro\n",
    "import time\n",
    "import json\n",
    "\n",
    "#### DOCUMENTATION VIA NUMPY ####\n",
    "# np.exp() => exponentiel calculation of each input array 's elements\n",
    "# particularly useful in neural networks to calculate the gradient of the error\n",
    "# .ravel() => flatten a table N-D in 1-D\n",
    "\n",
    "# hyperparameters\n",
    "H = 200  # number of hidden layer neurons\n",
    "batch_size = 10  # every how many episodes to do a param update?\n",
    "learning_rate = 1e-4\n",
    "gamma = 0.99  # discount factorpong for reward\n",
    "decay_rate = 0.99  # decay factor for RMSProp leaky sum of grad^2\n",
    "resume = False  # resume from previous checkpoint?\n",
    "render = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "filled-input",
   "metadata": {},
   "source": [
    "<h2> Hyperparamètre </h2>\n",
    "\n",
    "Paramètre d’ajustement de l’algorithme d’apprentissage dont la valeur est fixée avant le début du processus d’apprentissage. La différence avec un paramètre c’est ce que ce dernier est interne au réseau de neurones et va évoluer durant le processus d’entrainement, tandis que l’hyperparamètre est à l’inverse, un paramètre qui est une constante et est externe à ce réseau. On y retrouve des variables comme le batch size, le learning rate et le decay rate.\n",
    "\n",
    "- Le H correspond au nombre de neurone qui sont situés dans la couche cachée\n",
    "- Le batch size permet de diminuer le nombre d’échantillons qui vont être propagés à travers le réseau. Ça permet, tous les k épisodes (k = batch_size), de modifier le gradient (en faisant la moyenne des gradients du batch) durant la descente du gradient en affinant les paramètres. C’est notamment appliqué car ça permet de réduire la mémoire utilisée (ça n’utilisera qu’1/k ème de mémoire) et l’entrainement sera ainsi plus rapide.\n",
    "- Le learning rate est une valeur faible permettant d'éviter des oscillations du gradient lors de la rétropropagation\n",
    "- gamma: Discounted factor permettant de calculer le discount reward. (Voir discount reward)\n",
    "- decay date: Diminue l'importance du learning rate. En effet, il est important durant la phase d’apprentissage d’adapter le learning rate. Un learning rate important peut aider à converger rapidement au début, mais si le learning rate reste important dans les dernières étapes, il peut causer une divergence dans la descente de gradient. Il est donc important de pouvoir réduire le learning rate petit à petit durant la descente du gradient afin de pouvoir continuer à converger durant toute la phase d’apprentissage.\n",
    "- Le resume permet de reprendre ou pas là où on s'est arrété. \n",
    "- Le render permet d'afficher ou non le jeu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "studied-accreditation",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_reward_sum = []\n",
    "total_running_reward = []\n",
    "\n",
    "# model initialization\n",
    "# D = 80 * 80  # input dimensionality: 80x80 grid\n",
    "D = 120 * 80  # input dimensionality: 120x120 grid # First modification\n",
    "if resume:\n",
    "    model = pickle.load(open('save.p', 'rb'))\n",
    "else:\n",
    "    model = {}\n",
    "    model['W1'] = np.random.randn(H, D) / np.sqrt(D)  # \"Xavier\" initialization -> Initialisation pseudo-random neuron weight\n",
    "    model['W2'] = np.random.randn(H) / np.sqrt(H)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hydraulic-pavilion",
   "metadata": {},
   "source": [
    "<h2> Xavier Initilization </h2>\n",
    "\n",
    "Heuristique sur l'initialisation des poids dans les réseaux de neurones. Sa formule est la suivante :\n",
    "Le poids dans les réseaux à l’initialisation est important. \n",
    "En effet, si les poids ne sont pas correctement initialisés, les fonctions d’optimisation peuvent conduire nulle part, même après des centaines de milliers d’itérations.\n",
    "\n",
    "<h2> Réseaux de neurones </h2>\n",
    "\n",
    "Notre réseau de neurones renvoie une probabilité d’appuyer sur une touche. Pour cela, on utilise un réseau à deux couches de neurones. Pour chaque couche de neurones, nous utilisons une matrice qui contiendra les pondérations sur les entrées des neurones. Ces poids sont initialisés aléatoirement en utilisant la Xavier Initialization. Ces deux matrices sont appelées W1 pour la première couche et W2 pour la seconde couche. W1 est de dimension 9600* H, 9600 étant le nombre de pixels après la preprocessing et H étant un hyperparamètre correspondant au nombre de neurones de la couche cachée. W2 est de dimension H\\*1, 1 car W2 ne contient qu’un seul neurone. Il aurait été bien évidemment possible de créer plus de neurones, par exemple un neurone pour chaque touche, ce qui aurait donné une dimension de n\\*H, n étant le nombre de touches.\n",
    "\n",
    "<img src=\"img/policyForward.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "loose-layout",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_buffer = {k: np.zeros_like(v) for k, v in iter(model.items())}  # update buffers that add up gradients over a batch\n",
    "rmsprop_cache = {k: np.zeros_like(v) for k, v in iter(model.items())}  # rmsprop memory\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))  # sigmoid \"squashing\" function to interval [0,1]\n",
    "\n",
    "\n",
    "def prepro(I): # frame 240x160x3 uint8 frame\n",
    "    \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\" # -> Celeste : frame size 240x160x3 -> crop 120x80\n",
    "    # I = I[35:195]  # crop -> pas de crop\n",
    "    I = I[::2, ::2, 0]  # downsample by factor of 2\n",
    "    I[I == 144] = 0  # erase background (background type 1)\n",
    "    I[I == 109] = 0  # erase background (background type 2)\n",
    "    I[I != 0] = 1  # everything else (paddles, ball) just set to 1\n",
    "    return I.astype(np.float).ravel()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "toxic-blogger",
   "metadata": {},
   "source": [
    "<h2> Sigmoid </h2>\n",
    "\n",
    "Cette fonction permet d'écraser la somme des informations reçues par le neurone dans un intervalle [0,1]. Historiquement, dès les premières utilisations de réseaux de neurones, la fonction sigmoïde permettait d'interpréter au plus proche la fréquence d'envoi des informations en sortie d'un neurone.  \n",
    "Cependant, la fonction peut poser des problèmes lorsque les poids des neurones sont trop différents. Cela entraîne la perte du transport des informations au travers des neurones : ils sont désactivés. En effet, la réduction du gradient tend vers 0, c'est le Vanish gradient problem. Nous notons un autre problème car la fonction n'est pas centrée en zéro, ce qui peut amener à des mises à jour indésirables du gradient lors de la rétropropagation du gradient.\n",
    "\n",
    "<img src=\"img/SCHEMA_SIGMOIDE.png\" />\n",
    "\n",
    "<h2> Préprocessing </h2>\n",
    "\n",
    "Afin d'appliquer ce modèle à Celeste, nous avons commencé par découper chaque frame du jeu afin de ne garder que les seules informations nécessaires aux calculs de l'agent. Puis chaque frame est comparée à la frame précèdente pour obtenir un vecteur ne comportant que les changements d'une frame à une autre. Ce vecteur est donné au réseau de neurones pour calculer les probabilités des prochaines actions.\n",
    "\n",
    "<img src=\"img/SCHEMA_PERCEPTRON.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "confused-domain",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(r):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, r.size)):\n",
    "        if r[t] != 0: running_add = 0  # reset the sum, since this was a game boundary (pong specific!)\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "correct-customer",
   "metadata": {},
   "source": [
    "<h2> Discount reward </h2>\n",
    "\n",
    "Dans le Reinforcement Learning, le nombre total de rewards gagnés au cours d’un épisode est obtenu via l’équation :\n",
    "\n",
    "\n",
    "Où t est la longueur de l’épisode (qui peut être fini ou infini).\n",
    "C’est ce qui caractérise l’importance de prendre une action pour un agent, dans un état donné.\n",
    "Le problème de cette méthode, c’est que l’agent peut entrer dans une boucle où il collecte un nombre infini de rewards, et il ne sera soumis à aucune pression pour agir. Il peut passer par des millions de mauvais états si ça lui permet d’arriver dans un bon état où il pourra y rester pour toujours. $R = r_1 + r_2 + ... + r_t$\n",
    "\n",
    "Le concept de discounting est utilisé en Reinforcement Learning, où un paramètre appelé le « discount factor » est utilisé, représenté par γ et 0≤γ≤1 qui est un facteur de $r_t$. Par exemple, γ=0 rend l’agent myope, et ne cherche que les récompenses immédiates. γ=1 rend l’agent trop clairvoyant au point qu’il procrastinera dans l’accomplissement de l’objectif final. Mais une valeur γ $\\in$ `]0;1[` peut être utilisée pour s’assurer que l’agent n’est ni trop myope, ni trop clairvoyant. γ s’assure que l’agent prioritisera ses actions pour maximiser $R_t$ (à un instant t), obtenu par l’équation : $\\sum_{k=t}^T \\gamma^{(k-t)} k_{k}(s_{k},a_{k})$\n",
    "\n",
    "Permettant ainsi aux rewards obtenus au fil du temps d’être moins importants. Ainsi, un reward situé dans un futur lointain sera moins important qu’un reward que l’agent peut obtenir dans un futur proche. Cela aide l’agent à ne pas perdre de temps et à prioritiser ses actions. En pratique, γ $\\in$ `[0.9;0.99]` est typiquement utilisé dans la plupart des problèmes de RL.\n",
    "\n",
    "Les additive rewards sont surtout utilisés pour les problèmes à horizon fini, i.e un nombre discret d’étapes sur lesquelles optimiser, et les discounted rewards sont plus utilisés dans les problèmes à horizon infini, i.e optimiser sur un nombre infini d’étapes (ou au moins un nombre très large)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cosmetic-portland",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing h (hidden state) with W1 (first state)\n",
    "# to compute logp with the hidden state and the last state (W2).\n",
    "def policy_forward(x):\n",
    "    h = np.dot(model['W1'], x)\n",
    "    h[h < 0] = 0  # ReLU nonlinearity\n",
    "    logp = np.dot(model['W2'], h)\n",
    "    p = sigmoid(logp)\n",
    "    return p, h  # return probability of taking any actions, and hidden state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smoking-shadow",
   "metadata": {},
   "source": [
    "<h2> Policy forward </h2>\n",
    "\n",
    "On donne ces deux matrices à la fonction forward qui renvoie une prédiction avec un taux d’erreur plus ou moins grand.\n",
    "Comment ça fonctionne ?\n",
    "Notre réseau de neurones prendra l’état du jeu (c’est-à-dire le delta entre la current frame et la précédente frame) et fait un produit vectoriel avec la matrice W1. \n",
    "Le résultat de ce calcul linéaire donnera donc un vecteur qui est la couche cachée auquel nous appliquons la transformation non linéaire qui est la ReLu afin d’éliminer toutes les valeurs négatives. En effet, cette fonction retourne le max entre la valeur et 0.\n",
    "À ce stade, nous avons donc les valeurs des neurones de la couche cachée, que l’on multiplie par W2 pour calculer la deuxième couche. On applique une sigmoïde afin d’obtenir la probabilité de faire une action.\n",
    "La fonction forward renvoie donc cette probabilité ainsi que la couche cachée car elle servira pour la rétropropagation.\n",
    "\n",
    "<img src=\"img/forward_backward.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "stylish-gateway",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_backward(eph, epdlogp):\n",
    "    \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n",
    "    dW2 = np.dot(eph.T, epdlogp).ravel()\n",
    "    dh = np.outer(epdlogp, model['W2'])\n",
    "    dh[eph <= 0] = 0  # backpro prelu : calculation how to modify W1 and W2 in relation to the reward/penalty\n",
    "    dW1 = np.dot(dh.T, epx)\n",
    "    return {'W1': dW1, 'W2': dW2}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "martial-berlin",
   "metadata": {},
   "source": [
    "<h2> Policy backward </h2>\n",
    "\n",
    "Cette fonction fait le chemin inverse de la fonction forward, on calcule les pondérations des couches en entrée en fonction des sorties. Donc dans un premier temps, un produit vectoriel est effectué entre la transposée de la couche cachée et la nouvelle valeur de chaque bouton, cette valeur ayant été modifiée en prenant en compte les rewards, afin d’avoir les nouvelles pondérations pour W2. Ensuite, un produit matriciel entre ces nouvelles valeurs et la matrice W2 est appliqué. \n",
    "On applique la transformation non linéaire PReLU, qui est souvent utilisée dans la rétropropagation et qui dans notre cas met les valeurs de la nouvelle couche cachée à 0 lorsque la valeur de l’ancienne est inférieure ou égale à 0, avant d’appliquer enfin un produit vectoriel entre le nouveau vecteur ainsi créé et les valeurs de l’environnement, afin d’avoir les nouvelles pondérations de W1. \n",
    "\n",
    "<img src=\"img/backward.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "scientific-globe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeInFile(total_reward_sum, total_running_reward):\n",
    "    \"\"\"Write some rewards data in a file\"\"\"\n",
    "    data = {}\n",
    "    data['records'] = []\n",
    "\n",
    "    for i in range(len(total_reward_sum)):\n",
    "        data['records'].append({\n",
    "            'rewardSum': total_reward_sum[i],\n",
    "            'runningReward': total_running_reward[i]\n",
    "        })\n",
    "\n",
    "    with open('runRewardRecords.json', 'w') as outfile:\n",
    "        json.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selective-maple",
   "metadata": {},
   "source": [
    "<h2> Intégration d'un fichier JSON </h2>\n",
    "\n",
    "Cette fonction permet de récupérer les résultats des différents épisodes et de les stocker dans un fichier JSON.\n",
    "Avec cela on peut facilement faire un graphe comme celui-ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "framed-correlation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "def show_graph():\n",
    "\n",
    "    total_reward_sum = []\n",
    "    total_running_rewards = []\n",
    "\n",
    "    with open('records/runRewardRecords.json') as json_file:\n",
    "        file_data = json.load(json_file)\n",
    "        for elem in file_data['records']:\n",
    "            total_reward_sum.append(elem['rewardSum'])\n",
    "            total_running_rewards.append(elem['runningReward'])\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    #plt.scatter(total_reward_sum, total_running_rewards, \"ko-\")\n",
    "    plt.scatter(total_reward_sum, total_running_rewards, ls=\"-\")\n",
    "    ax.set_title('Evolution des rewards au fil des runs')\n",
    "    ax.set_xlabel('Reward sum')\n",
    "    ax.set_ylabel('Running reward')\n",
    "    plt.show()\n",
    "    \n",
    "show_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handy-quilt",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-707c20f44705>:16: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  return I.astype(np.float).ravel()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resetting env. episode reward total was 0.200000. running mean: 0.200000\n",
      "ep 1: game finished, reward: -1.000000\n"
     ]
    }
   ],
   "source": [
    "env = retro.make(game=\"Celeste-GBA\", use_restricted_actions=retro.Actions.MULTI_DISCRETE)\n",
    "observation = env.reset()\n",
    "prev_x = None  # used in computing the difference frame\n",
    "xs, hs1, hs2, hs3, dlogps1, dlogps2, dlogps3, drs = [], [], [], [], [], [], [], []\n",
    "running_reward = None\n",
    "reward_sum = 0\n",
    "episode_number = 0\n",
    "actionIndex = 0.0\n",
    "t0 = time.time()\n",
    "while True:\n",
    "    if render: env.render()\n",
    "\n",
    "    actionIndex += 0.01\n",
    "\n",
    "    # preprocess the observation, set input to network to be difference image\n",
    "    cur_x = prepro(observation)\n",
    "    x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
    "    prev_x = cur_x\n",
    "\n",
    "    # forward the policy network and sample an action from the returned probability\n",
    "    aprob1, h1 = policy_forward(x)\n",
    "    aprob2, h2 = policy_forward(x)\n",
    "    aprob3, h3 = policy_forward(x)\n",
    "\n",
    "    # record various intermediates (needed later for backprop)\n",
    "    xs.append(x)  # observation\n",
    "    hs1.append(h1)  # hidden state\n",
    "    hs2.append(h2)  # hidden state\n",
    "    hs3.append(h3)  # hidden state\n",
    "    \n",
    "    # Action choice with ranges\n",
    "\n",
    "    # elem1 = 0\n",
    "    # if aprob1 <= 0.49:\n",
    "    #     elem1 = 0\n",
    "    # elif aprob1 <= 0.50:\n",
    "    #     elem1 = 1\n",
    "    # else:\n",
    "    #     elem1 = 2\n",
    "\n",
    "        \n",
    "    # elem2 = 0\n",
    "    # if aprob2 <= 0.49:\n",
    "    #     elem2 = 0\n",
    "    # elif aprob2 <= 0.50:\n",
    "    #     elem2 = 1\n",
    "    # else:\n",
    "    #     elem2 = 2\n",
    "\n",
    "    # elem3 = 0\n",
    "    # if aprob3 <= 0.485:\n",
    "    #     elem3 = 0\n",
    "    # elif aprob3 <= 0.495:\n",
    "    #     elem3 = 1\n",
    "    # elif aprob3 <= 0.505:\n",
    "    #     elem3 = 2\n",
    "    # else:\n",
    "    #     elem3 = 4\n",
    "\n",
    "\n",
    "    # Action choice\n",
    "    elem1 = 1 if np.random.uniform() < aprob1 else 2\n",
    "    elem2 = 1 if np.random.uniform() < aprob2 else 2\n",
    "    elem31 = 1 if np.random.uniform() < aprob3 else 0\n",
    "    elem32 = 2 if np.random.uniform() < aprob3 else 0\n",
    "\n",
    "    if elem31 != 0 and elem32 != 0:\n",
    "        elem3 = 4\n",
    "    elif elem31 == 1:\n",
    "        elem3 = 1\n",
    "    elif elem32 == 2:\n",
    "        elem3 = 2 \n",
    "    else:\n",
    "        elem3 = 0\n",
    "\n",
    "    # Fake labe http://cs231n.github.io/neural-networks-2/#losses\n",
    "    dlogps1.append(elem1 - aprob1)\n",
    "    dlogps2.append(elem2 - aprob2)\n",
    "    dlogps3.append(elem3 - aprob3)\n",
    "\n",
    "    # Action\n",
    "    a = [elem1, elem2, elem3, 0]\n",
    "\n",
    "    # step the environment and get new measurements\n",
    "    observation, reward, done, info = env.step(a)\n",
    "\n",
    "    drs.append(reward)  # record reward (has to be done after we call step() to get reward for previous action)\n",
    "    \n",
    "    if done:  # an episode finished\n",
    "        t1 = time.time()             \n",
    "        if t1-t0 > 1:        \n",
    "            reward_sum += 0.1*((t1-t0)//2)    \n",
    "\n",
    "        episode_number += 1\n",
    "        \n",
    "\n",
    "        # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
    "        epx = np.vstack(xs)\n",
    "        eph1 = np.vstack(hs1)\n",
    "        eph2 = np.vstack(hs2)\n",
    "        eph3 = np.vstack(hs3)\n",
    "        epdlogp1 = np.vstack(dlogps1)\n",
    "        epdlogp2 = np.vstack(dlogps2)\n",
    "        epdlogp3 = np.vstack(dlogps3)\n",
    "        epr = np.vstack(drs)\n",
    "        xs, hs1, hs2, hs3, dlogps1, dlogps2, dlogps3, drs = [], [], [], [], [], [], [], []  # reset array memory\n",
    "\n",
    "        # compute the discounted reward backwards through time\n",
    "        discounted_epr = discount_rewards(epr)\n",
    "        # standardize the rewards pong to be unit normal (helps control the gradient estimator variance)\n",
    "        discounted_epr -= np.mean(discounted_epr)\n",
    "        discounted_epr /= np.std(discounted_epr)\n",
    "\n",
    "        epdlogp1 *= discounted_epr  # modulate the gradient with advantage (PG magic happens right here.)\n",
    "        epdlogp2 *= discounted_epr  # modulate the gradient with advantage (PG magic happens right here.)\n",
    "        epdlogp3 *= discounted_epr  # modulate the gradient with advantage (PG magic happens right here.)\n",
    "        grad1 = policy_backward(eph1, epdlogp1)\n",
    "        grad2 = policy_backward(eph2, epdlogp2)\n",
    "        grad3 = policy_backward(eph3, epdlogp3)\n",
    "        for k in model: grad_buffer[k] += grad1[k]  # accumulate grad over batch\n",
    "        for k in model: grad_buffer[k] += grad2[k]  # accumulate grad over batch\n",
    "        for k in model: grad_buffer[k] += grad3[k]  # accumulate grad over batch\n",
    "\n",
    "        # perform rmsprop parameter update every batch_size episodes\n",
    "        if episode_number % batch_size == 0:\n",
    "            for k, v in iter(model.items()):\n",
    "                g = grad_buffer[k]  # gradient\n",
    "                rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g ** 2\n",
    "                model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
    "                grad_buffer[k] = np.zeros_like(v)  # reset batch gradient buffer\n",
    "\n",
    "        # boring book-keeping\n",
    "        running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "        print('resetting env. episode reward total was %f. running mean: %f' % (reward_sum, running_reward))\n",
    "        total_reward_sum.append(reward_sum)\n",
    "        total_running_reward.append(running_reward)\n",
    "\n",
    "        if episode_number % 100 == 0: pickle.dump(model, open('save.p', 'wb'))\n",
    "        reward_sum = 0\n",
    "        observation = env.reset()  # reset env\n",
    "        prev_x = None\n",
    "\n",
    "    if reward != 0:  # Pong has either +1 or -1 reward exactly when game ends.\n",
    "        print('ep %d: game finished, reward: %f' % (episode_number, reward))\n",
    "        writeInFile(total_reward_sum, total_running_reward)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intellectual-interstate",
   "metadata": {},
   "source": [
    "<h2> Les inputs </h2>\n",
    "\n",
    "Sur un simulateur GBA, nous avons 3 types d’inputs pouvant être acceptés:\n",
    "- Les inputs Discrets où chaque action ou combo correspondent à une touche. Ce type d’input est associé à une chaine de caractères («UP» «DOWN» «LEFT» ...)\n",
    "- Les inputs MultiDiscrets où chaque action correspond à une touche et un combo à deux touches. Ce type d'input que nous utilisons pour l'IA est associé à un tableau d'entiers et nous y reviendrons juste après.\n",
    "- Les inputs MultiBinaires où chaque action correspond à une touche et un combo correspond à deux touches bind sur une seule. Ce type d'input est associé à un nombre de 0 à 11 pour le simulateur GBA.\n",
    "\n",
    "Ici, on utilise les inputs MultiDiscrets et on récupère les résultats de chaque réseau de neurones afin de décider quelle action effectuer. Sous forme MultiDiscret, on doit créer un tableau de 4 éléments (car il y a 4 catégories d'actions sous forme MultiDiscret) et où on place chaque type d'action dans une case.\n",
    "Par exemple, pour le choix entre la droite et la gauche, on récupère la valeur calculée par le réseau de neurones associé et on le compare avec un nombre aléatoire choisi uniformément, puis en fonction du résultat, on décide d'aller à droite ou à gauche. L'axe horizontal correspond à la première catégorie d'actions donc on met ce résultat dans la première case du tableau.\n",
    "\n",
    "On remarque qu'on n'utilise pas l'action `Select`, `L` et `R` pour l'IA donc on ignore ces éléments.\n",
    "\n",
    "On peut également choisir les actions par plage de probabilité comme sur le code en commentaire. \n",
    "Cependant, il faut faire une analyse précise des plages à sélectionner pour avoir un agent convenable.\n",
    "\n",
    "<img src=\"img/inputs.PNG\" />\n",
    "\n",
    "<h2> RMSProp </h2>\n",
    "\n",
    "Ce code utilise l’algorithme Root Mean Square Propagation (RMSProp) comme fonction d’optimisation de la descente de gradient stochastique. \n",
    "Sa formule est la suivante : $v(w,t) = yv(w,t -1) + 1 (1 - y)(\\nabla Q_1(w))^2$\n",
    "\n",
    "- $\\lambda$: decay rate\n",
    "- $\\nabla Q_1(w)$: gradient buffer du batch\n",
    "- $v(w,t -1)$: La mémoire rmsprop du batch\n",
    "\n",
    "Et la fonction de la mise à jour des paramètres dans cet algorithme est: $w = \\frac{\\eta}{\\sqrt{(w, t)}} \\nabla Q_i(w)$\n",
    "\n",
    "- $\\eta$: learning rate\n",
    "- $\\nabla Q_1(w)$: dragient buffer du batch\n",
    "- $w$: la pondération d'un neurone\n",
    "\n",
    "L’algorithme permet donc de converger plus rapidement en diminuant les oscillations grâce à une bonne adaptation du learning rate. Cet algorithme est appliqué tous les 10 épisodes (le batch size). \n",
    "\n",
    "<img src=\"img/rmsprop.PNG\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dynamic-roads",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
